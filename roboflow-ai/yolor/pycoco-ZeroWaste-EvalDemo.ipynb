{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.google_utils import attempt_load\n",
    "from utils.datasets import create_dataloader\n",
    "from utils.general import coco80_to_coco91_class, check_dataset, check_file, check_img_size, box_iou, \\\n",
    "    non_max_suppression, scale_coords, xyxy2xywh, xywh2xyxy, clip_coords, set_logging, increment_path\n",
    "from utils.loss import compute_loss\n",
    "from utils.metrics import ap_per_class\n",
    "from utils.plots import plot_images, output_to_target\n",
    "from utils.torch_utils import select_device, time_synchronized\n",
    "\n",
    "from models.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_classes(path):\n",
    "    # Loads *.names file at 'path'\n",
    "    with open(path, 'r') as f:\n",
    "        names = f.read().split('\\n')\n",
    "    return list(filter(None, names))  # filter removes empty strings (such as last line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16,\n",
    "imgsz=640,\n",
    "conf_thres=0.5,\n",
    "iou_thres=0.6,  # for NMS\n",
    "save_json=True,\n",
    "single_cls=False,\n",
    "augment=False,\n",
    "verbose=True,\n",
    "model=None,\n",
    "dataloader=None,\n",
    "save_dir=Path(''),  # for saving images\n",
    "save_txt=False,  # for auto-labelling\n",
    "save_conf=False,\n",
    "plots=True,\n",
    "log_imgs=0  # number of logged images\n",
    "task = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You must run this one, the follwoing code relys on this opt object\n",
    "from argparse import Namespace\n",
    "opt = Namespace(augment=False, batch_size=32, cfg='./cfg/yolor_p6.cfg', conf_thres=0.5, data='../zero-waste-1/data.yaml', device='0', exist_ok=False, img_size=416, iou_thres=0.65, name='exp', names='./data/zerowaste.names', project='../runs/test', save_conf=True, save_json=True, save_txt=False, single_cls=False, task='test', verbose=True, weights=['../runs/train/yolor_p6_2022_03_26-10_44_07/weights/best_overall.pt'])\n",
    "\n",
    "# ===========================> The following are some params will be passed to test() funciton\n",
    "weights=[\"../runs/train/yolor_p6_2022_03_26-10_44_07/weights/best_overall.pt\"]\n",
    "data = \"../zero-waste-1/data.yaml\"\t# Because this file will load some utils module, so this file must be put under yolor folder\n",
    "batch_size=32\n",
    "\n",
    "imgsz=640\n",
    "# conf_thres=0.5\t# object confidence threshold\n",
    "# iou_thres=0.6  # IOU threshold for NMS, or .65\n",
    "task='test'\n",
    "device='0'\t# help='cuda device, i.e. 0 or 0,1,2,3 or cpu'\n",
    "single_cls=False\n",
    "augment=False\n",
    "verbose=True\n",
    "# save_txt=False\t# for auto-labelling\n",
    "# save_conf=True\n",
    "save_json=True\n",
    "\n",
    "project='../runs/test'\n",
    "name='exp'\n",
    "exist_ok=False\n",
    "cfg='./cfg/yolor_p6.cfg'\n",
    "names='./data/zerowaste.names'\n",
    "\n",
    "# model=None\n",
    "log_imgs=0  \t# number of logged imag\n",
    "save_dir=Path('./')  # for saving images\n",
    "plots=True\n",
    "dataloader=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('.')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using torch 1.10.1 CUDA:0 (NVIDIA GeForce RTX 2070 with Max-Q Design, 8191MB)\n",
      "\n",
      "Model Summary: 665 layers, 36854616 parameters, 36854616 gradients\n"
     ]
    }
   ],
   "source": [
    "set_logging()\n",
    "device = select_device(opt.device, batch_size=opt.batch_size)\n",
    "save_txt = opt.save_txt  # save *.txt labels\n",
    "\n",
    "# Directories: where all the logging file will be saved for this experiment\n",
    "save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run\n",
    "(save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "# Load model\n",
    "model = Darknet(opt.cfg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ckpt = torch.load(opt.weights[0], map_location=device)  # load checkpoint\n",
    "    ckpt['model'] = {k: v for k, v in ckpt['model'].items() if model.state_dict()[k].numel() == v.numel()}\n",
    "    model.load_state_dict(ckpt['model'], strict=False)\n",
    "except:\n",
    "    load_darknet_weights(model, opt.weights[0])\n",
    "imgsz = check_img_size(imgsz, s=64)  # check img_size\n",
    "\n",
    "# Half: half precision if we are running on GPU\n",
    "half = device.type != 'cpu'\n",
    "if half:\n",
    "    model.half()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure\n",
    "yaml_dir='../zero-waste-1/data.yaml'\n",
    "model.eval()\n",
    "is_coco = yaml_dir.endswith('coco.yaml')  # is COCO dataset\n",
    "with open(yaml_dir) as f:\n",
    "    data = yaml.load(f, Loader=yaml.FullLoader)  # model dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'names': ['cardboard', 'metal', 'rigid_plastic', 'soft_plastic'], 'nc': 4, 'train': '../zero-waste-1/train/images', 'val': '../zero-waste-1/valid/images', 'test': '../zero-waste-1/test/images'}\n"
     ]
    }
   ],
   "source": [
    "print(data)\t# Making sure you are running this file under yolor folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dataset(data)  # check\n",
    "nc = 1 if single_cls else int(data['nc'])  # number of classes\n",
    "iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n",
    "niou = iouv.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\59384\\anaconda3\\envs\\latest_tf\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Scanning labels ..\\zero-waste-1\\test\\labels.cache3 (8 found, 0 missing, 0 empty, 0 duplicate, for 8 images): 8it [00:00, 1599.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# Logging\n",
    "log_imgs, wandb = min(log_imgs, 100), None  # ceil\n",
    "try:\n",
    "    import wandb  # Weights & Biases\n",
    "except ImportError:\n",
    "    log_imgs = 0\n",
    "\n",
    "# Dataloader\n",
    "img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
    "_ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n",
    "path = data['test'] if opt.task == 'test' else data['val']  # path to val/test images\n",
    "dataloader = create_dataloader(path, imgsz, batch_size, 64, opt, pad=0.5, rect=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../zero-waste-1/test/images'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = 0\n",
    "try:\n",
    "    names = model.names if hasattr(model, 'names') else model.module.names\n",
    "except:\n",
    "    names = load_classes(opt.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cardboard', 'metal', 'rigid_plastic', 'soft_plastic']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    jdict: json dictionary, what we use to save as json file, for futher evaluation\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco91class = coco80_to_coco91_class()  # Return a int list, has value from 1 to 90\n",
    "s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Targets', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')    # ==> '               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95'\n",
    "p, r, f1, mp, mr, map50, map, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.\n",
    "loss = torch.zeros(3, device=device)\n",
    "jdict, stats, ap, ap_class, wandb_images = [], [], [], [], []\n",
    "\"\"\"\n",
    "    jdict: json dictionary, what we use to save as json file, for futher evaluation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_get_iterator',\n",
       " '_index_sampler',\n",
       " '_is_protocol',\n",
       " '_iterator',\n",
       " 'batch_sampler',\n",
       " 'batch_size',\n",
       " 'check_worker_number_rationality',\n",
       " 'collate_fn',\n",
       " 'dataset',\n",
       " 'drop_last',\n",
       " 'generator',\n",
       " 'iterator',\n",
       " 'multiprocessing_context',\n",
       " 'num_workers',\n",
       " 'persistent_workers',\n",
       " 'pin_memory',\n",
       " 'prefetch_factor',\n",
       " 'sampler',\n",
       " 'timeout',\n",
       " 'worker_init_fn']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dataloader)[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'Tensor' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_67356/1076642961.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Run NMS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_synchronized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnon_max_suppression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minf_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf_thres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf_thres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miou_thres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miou_thres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mt1\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtime_synchronized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Statistics per image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Zhengqi Dong\\Education\\BU\\2022_Spring\\EC523_DeepLearning\\Project\\EC523_DL_CV_Project\\roboflow-ai\\yolor\\utils\\general.py\u001b[0m in \u001b[0;36mnon_max_suppression\u001b[1;34m(prediction, conf_thres, iou_thres, merge, classes, agnostic)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[0mnc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m5\u001b[0m  \u001b[1;31m# number of classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m     \u001b[0mxc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mconf_thres\u001b[0m  \u001b[1;31m# candidates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;31m# Settings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'Tensor' and 'tuple'"
     ]
    }
   ],
   "source": [
    "training = False\n",
    "for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):\n",
    "    img = img.to(device, non_blocking=True)\n",
    "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    targets = targets.to(device)\n",
    "    nb, _, height, width = img.shape  # batch size, channels, height, width\n",
    "    whwh = torch.Tensor([width, height, width, height]).to(device)\n",
    "    # Disable gradients\n",
    "    with torch.no_grad():\n",
    "        # Run model\n",
    "        t = time_synchronized()\n",
    "        inf_out, train_out = model(img, augment=augment)  # inference and training outputs\n",
    "        t0 += time_synchronized() - t\n",
    "        # Compute loss\n",
    "        if training:  # if model has loss hyperparameters\n",
    "            loss += compute_loss([x.float() for x in train_out], targets, model)[1][:3]  # box, obj, cls\n",
    "        # Run NMS\n",
    "        t = time_synchronized()\n",
    "        output = non_max_suppression(inf_out, conf_thres=conf_thres, iou_thres=iou_thres)\n",
    "        t1 += time_synchronized() - t\n",
    "    # Statistics per image\n",
    "    for si, pred in enumerate(output):  # si is index, pred is the predicted bbox, e.g., [[269.00000, 146.25000, 328.50000, 285.00000,   0.68604,   0.00000], so [x, y, w, h, score, category]\n",
    "        labels = targets[targets[:, 0] == si, 1:]\n",
    "        nl = len(labels)\n",
    "        tcls = labels[:, 0].tolist() if nl else []  # target class\n",
    "        seen += 1\n",
    "        if len(pred) == 0:\n",
    "            if nl:\n",
    "                stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
    "            continue\n",
    "        # Append to text file\n",
    "        path = Path(paths[si])\n",
    "        if save_txt:\n",
    "            gn = torch.tensor(shapes[si][0])[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            x = pred.clone()\n",
    "            x[:, :4] = scale_coords(img[si].shape[1:], x[:, :4], shapes[si][0], shapes[si][1])  # to original\n",
    "            for *xyxy, conf, cls in x:\n",
    "                xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
    "                with open(save_dir / 'labels' / (path.stem + '.txt'), 'a') as f:\n",
    "                    f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "        # W&B logging\n",
    "        if plots and len(wandb_images) < log_imgs:\n",
    "            box_data = [{\"position\": {\"minX\": xyxy[0], \"minY\": xyxy[1], \"maxX\": xyxy[2], \"maxY\": xyxy[3]},\n",
    "                         \"class_id\": int(cls),\n",
    "                         \"box_caption\": \"%s %.3f\" % (names[int(cls)], conf),\n",
    "                         \"scores\": {\"class_score\": conf},\n",
    "                         \"domain\": \"pixel\"} for *xyxy, conf, cls in pred.tolist()]\n",
    "            boxes = {\"predictions\": {\"box_data\": box_data, \"class_labels\": names}}\n",
    "            # print(f\"img[si]: {img[si]}, boxes: {boxes}, path.name: {path.name}\")\n",
    "            # print(f\"wandb.Image(img[si], boxes=boxes, caption=path.name): {wandb.Image(img[si], boxes=boxes, caption=path.name)}\")\n",
    "            wandb_images.append(wandb.Image(img[si], boxes=boxes, caption=path.name))\n",
    "        # Clip boxes to image bounds\n",
    "        clip_coords(pred, (height, width))\n",
    "        # Append to pycocotools JSON dictionary\n",
    "        if save_json:\n",
    "            # [{\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}, ...\n",
    "            image_id = int(path.stem) if path.stem.isnumeric() else path.stem\n",
    "            # Note, if we don't get numeric() value, that it's taking filename, which will give us something like, \"image_id\": \"12_frame_024600_PNG.rf.7f0302039ef984cd4bcdc54cacdc48fd\" --> This is bad, and it can't be evaluate with cocoAPI  ==> Results do not correspond to current coco set\n",
    "            box = pred[:, :4].clone()  # xyxy\n",
    "            scale_coords(img[si].shape[1:], box, shapes[si][0], shapes[si][1])  # to original shape\n",
    "            box = xyxy2xywh(box)  # xywh\n",
    "            box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\n",
    "            for p, b in zip(pred.tolist(), box.tolist()):\n",
    "                jdict.append({'image_id': image_id,\n",
    "                              'category_id': coco91class[int(p[5])] if is_coco else int(p[5]),\n",
    "                              'bbox': [round(x, 3) for x in b],\n",
    "                              'score': round(p[4], 5)})\n",
    "        # Assign all predictions as incorrect\n",
    "        correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool, device=device)\n",
    "        if nl:\n",
    "            detected = []  # target indices\n",
    "            tcls_tensor = labels[:, 0]\n",
    "            # target boxes\n",
    "            tbox = xywh2xyxy(labels[:, 1:5]) * whwh\n",
    "            # Per target class\n",
    "            for cls in torch.unique(tcls_tensor):\n",
    "                ti = (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)  # prediction indices\n",
    "                pi = (cls == pred[:, 5]).nonzero(as_tuple=False).view(-1)  # target indices\n",
    "                # Search for detections\n",
    "                if pi.shape[0]:\n",
    "                    # Prediction to target ious\n",
    "                    ious, i = box_iou(pred[pi, :4], tbox[ti]).max(1)  # best ious, indices\n",
    "                    # Append detections\n",
    "                    detected_set = set()\n",
    "                    for j in (ious > iouv[0]).nonzero(as_tuple=False):\n",
    "                        d = ti[i[j]]  # detected target\n",
    "                        if d.item() not in detected_set:\n",
    "                            detected_set.add(d.item())\n",
    "                            detected.append(d)\n",
    "                            correct[pi[j]] = ious[j] > iouv  # iou_thres is 1xn\n",
    "                            if len(detected) == nl:  # all targets already located in image\n",
    "                                break\n",
    "        # Append statistics (correct, conf, pcls, tcls)\n",
    "        stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n",
    "    # Plot images\n",
    "    if plots and batch_i < 3:\n",
    "        print(f\"=========> type(output): {type(output)}\")\n",
    "        output = [i.cpu().detach().numpy()  for i in output]\n",
    "        f = save_dir / f'test_batch{batch_i}_labels.jpg'  # filename\n",
    "        plot_images(img, targets, paths, f, names)  # labels\n",
    "        f = save_dir / f'test_batch{batch_i}_pred.jpg'\n",
    "        plot_images(img, output_to_target(output , width, height), paths, f, names)  # predictions\n",
    "        # f = save_dir\n",
    "        # plot_results(save_dir=save_dir)  # predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_json = glob.glob(\"/projectnb/dl523/students/dong760/roboflow-ai/test/_annotations.coco.json\")[0]\n",
    "# importing the module\n",
    "pred_json = \"../runs/test/exp43/best_overall_predictions.json\"\n",
    "import json\n",
    " \n",
    "# Opening JSON file\n",
    "f1 = open(anno_json)\n",
    "f2 = open(pred_json)\n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "gT_dict = json.load(f1)\n",
    "jdict = json.load(f2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converer_dict = gT_dict[\"images\"]\n",
    "suffix = converer_dict[0][\"file_name\"].split('.')[-1]\n",
    "for i in range(len(jdict)):\n",
    "    image_id = jdict[i]['image_id'] + \".\" + suffix\n",
    "    if not image_id.isnumeric():\n",
    "        flag = True # Mark as incorrect image_id\n",
    "    for item in converer_dict:\n",
    "        # If we found a match, and the current image_id is marked as incorrect format ==> We will correct it\n",
    "        if flag and image_id == item[\"file_name\"]:\n",
    "            print(f\"jdict[i]['image_id']: {jdict[i]['image_id']}, item['id']: {item['id']}\")\n",
    "            jdict[i]['image_id'] = item['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'08_frame_011500_PNG.rf.fa2aa81dc2839b709ca7ce5e9feeed9b'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jdict[1][\"image_id\"].isnumeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jpg'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "converer_dict[428][\"file_name\"].split('.')[-1]\n",
    "# for item in converer_dict:\n",
    "#     # If we found a match, and the current image_id is marked as incorrect format ==> We will correct it\n",
    "#     if flag and image_id == item[\"file_name\"]:\n",
    "#         jdict[i][\"image_id\"] = item[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.22s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.45s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.11s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "================> Start Printing evaluated result:\n",
      "[          0           0           0           0           0           0           0           0           0           0           0           0]\n",
      "================> End of Printing!\n"
     ]
    }
   ],
   "source": [
    "pred_json = \"../runs/test/exp51/best_overall_predictions.json\"\n",
    "# anno_json = glob.glob(\"coco/annotations/instances_val*.json\")[0]  \n",
    "anno_json = glob.glob(\"/projectnb2/dl523/students/dong760/zerowaste_dataset/zerowaste-f-final/test/labels.json\")[0]  # finding the annotations json \n",
    "# anno_json = glob.glob(\"/projectnb/dl523/students/dong760/roboflow-ai/test/_annotations.coco.json\")[0]  # finding the annotations json \n",
    "is_coco = False\n",
    "try:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPIpycocoEvalDemo.ipynb\n",
    "    from pycocotools.coco import COCO\n",
    "    from pycocotools.cocoeval import COCOeval\n",
    "    anno = COCO(anno_json)  # init annotations api\n",
    "    pred = anno.loadRes(pred_json)  # init predictions api\n",
    "    # imgIds=sorted(anno.getImgIds())\n",
    "    # imgIds=imgIds[0:100]\n",
    "    # imgId = imgIds[np.random.randint(100)]\n",
    "    eval = COCOeval(anno, pred, 'bbox')\n",
    "    if is_coco:\n",
    "        eval.params.imgIds = [int(Path(x).stem) for x in dataloader.dataset.img_files]  # image IDs to evaluate\n",
    "    eval.evaluate()\n",
    "    eval.accumulate()\n",
    "    eval.summarize()\n",
    "    map, map50 = eval.stats[:2]  # update results (mAP@0.5:0.95, mAP@0.5)\n",
    "    print(\"================> Start Printing evaluated result:\")\n",
    "    print(eval.stats)\n",
    "    print(\"================> End of Printing!\")\n",
    "except Exception as e:\n",
    "    print('ERROR: pycocotools unable to run: %s' % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno.getCatIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=8.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#initialize COCO ground truth api\n",
    "dataDir='../'\n",
    "dataType='val2014'\n",
    "annFile = '%s/annotations/%s_%s.json'%(dataDir,prefix,dataType)\n",
    "cocoGt=COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...     \n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#initialize COCO detections api\n",
    "resFile='%s/results/%s_%s_fake%s100_results.json'\n",
    "resFile = resFile%(dataDir, prefix, dataType, annType)\n",
    "cocoDt=cocoGt.loadRes(resFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgIds=sorted(cocoGt.getImgIds())\n",
    "imgIds=imgIds[0:100]\n",
    "imgId = imgIds[np.random.randint(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running per image evaluation...      \n",
      "DONE (t=0.46s).\n",
      "Accumulating evaluation results...   \n",
      "DONE (t=0.38s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.505\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.697\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.573\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.586\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.519\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.501\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.387\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.594\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.595\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.640\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.566\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.564\n"
     ]
    }
   ],
   "source": [
    "# running evaluation\n",
    "cocoEval = COCOeval(cocoGt,cocoDt,annType)\n",
    "cocoEval.params.imgIds  = imgIds\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca0b83cf779738b65c72b916252aecc4e56a1a3d2993a7c32a45f1b5451d7c5a"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
