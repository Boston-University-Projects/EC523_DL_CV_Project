
     Epoch   gpu_mem       box       obj       cls     total   targets  img_size
       9/9     3.41G   0.02753    0.1232 0.0007323    0.1515        40       448: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 394/394 [00:44<00:00,  8.91it/s]
               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95:   0%|                                                                                                     | 0/57 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "./yolor/train.py", line 537, in <module>
    train(hyp, opt, device, tb_writer, wandb)
  File "./yolor/train.py", line 336, in train
    results, maps, times = test.test(opt.data,
  File "/projectnb2/dl523/students/dong760/roboflow-ai/yolor/test.py", line 163, in test
    box_data = [{"position": {"minX": xyxy[0], "minY": xyxy[1], "maxX": xyxy[2], "maxY": xyxy[3]},
  File "/projectnb2/dl523/students/dong760/roboflow-ai/yolor/test.py", line 165, in <listcomp>
    "box_caption": "%s %.3f" % (names[cls], conf),
TypeError: list indices must be integers or slices, not float

wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      metrics/mAP_0.5 ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà
wandb: metrics/mAP_0.5:0.95 ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñà
wandb:    metrics/precision ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñà
wandb:       metrics/recall ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñà
wandb:       train/box_loss ‚ñà‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ
wandb:       train/cls_loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train/obj_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:         val/box_loss ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ
wandb:         val/cls_loss ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:         val/obj_loss ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá
wandb:                x/lr0 ‚ñÅ‚ñÖ‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÅ
wandb:                x/lr1 ‚ñÅ‚ñÖ‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÅ
wandb:                x/lr2 ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      metrics/mAP_0.5 0.35513
wandb: metrics/mAP_0.5:0.95 0.24088
wandb:    metrics/precision 0.2357
wandb:       metrics/recall 0.47993
wandb:       train/box_loss 0.02897
wandb:       train/cls_loss 0.00078
wandb:       train/obj_loss 0.12614
wandb:         val/box_loss 0.02736
wandb:         val/cls_loss 0.00078
wandb:         val/obj_loss 0.1158
wandb:                x/lr0 0.00365
wandb:                x/lr1 0.00365
wandb:                x/lr2 0.00365
wandb: 
wandb: Synced yolor_p64: https://wandb.ai/dragogo/YOLOR/runs/2wwvkhdo
wandb: Synced 6 W&B file(s), 4 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220325_230255-2wwvkhdo/logs




(dl_env)dong760@scc-203:/projectnb/dl523/students/dong760/roboflow-ai$ python ./yolor/train.py --batch-size 8 --img 416 416 --data ./zero-waste-1/data.yaml --cfg ./yolor/cfg/yolov4_csp_x.cfg --weights './yolor/weights/yolov4.weights' --device 0 --name yolov4 --hyp './yolor/data/hyp.scratch.1280.yaml' --epochs 2
Using torch 1.9.0+cu111 CUDA:0 (Tesla V100-SXM2-16GB, 16160MB)

Namespace(adam=False, batch_size=8, bucket='', cache_images=False, cfg='./yolor/cfg/yolov4_csp_x.cfg', data='./zero-waste-1/data.yaml', device='0', epochs=2, evolve=False, exist_ok=False, global_rank=-1, hyp='./yolor/data/hyp.scratch.1280.yaml', image_weights=False, img_size=[416, 416], local_rank=-1, log_imgs=16, multi_scale=False, name='yolov4', noautoanchor=False, nosave=False, notest=False, project='runs/train', rect=False, resume=False, save_dir='runs/train/yolov42', single_cls=False, sync_bn=False, total_batch_size=8, weights='./yolor/weights/yolov4.weights', workers=8, world_size=1)
Start Tensorboard with "tensorboard --logdir runs/train", view at http://0.0.0.0:6006/
Hyperparameters {'lr0': 0.01, 'lrf': 0.2, 'momentum': 0.937, 'weight_decay': 0.0005, 'warmup_epochs': 3.0, 'warmup_momentum': 0.8, 'warmup_bias_lr': 0.1, 'box': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.5, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'mosaic': 1.0, 'mixup': 0.0}
WARNING: smart bias initialization failure.
WARNING: smart bias initialization failure.
WARNING: smart bias initialization failure.
Model Summary: 611 layers, 99751053 parameters, 99751053 gradients
Optimizer groups: 137 .bias, 137 conv.weight, 134 other
wandb: Currently logged in as: dragogo (use `wandb login --relogin` to force relogin)
wandb: Tracking run with wandb version 0.12.11
wandb: Run data is saved locally in /projectnb2/dl523/students/dong760/roboflow-ai/wandb/run-20220325_233519-1pe54vcd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run yolov42
wandb: ‚≠êÔ∏è View project at https://wandb.ai/dragogo/YOLOR
wandb: üöÄ View run at https://wandb.ai/dragogo/YOLOR/runs/1pe54vcd
WARNING: --img-size 416 must be multiple of max stride 64, updating to 448
WARNING: --img-size 416 must be multiple of max stride 64, updating to 448
Scanning labels zero-waste-1/train/labels.cache3 (3092 found, 0 missing, 56 empty, 1 duplicate, for 3148 images): 3148it [00:00, 17004.28it/s]
Scanning labels zero-waste-1/valid/labels.cache3 (876 found, 0 missing, 24 empty, 0 duplicate, for 900 images): 900it [00:00, 15836.79it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
Image sizes 448 train, 448 test
Using 8 dataloader workers
Logging results to runs/train/yolov42
Starting training for 2 epochs...

     Epoch   gpu_mem       box       obj       cls     total   targets  img_size
  0%|                                                                                                                                                                                                  | 0/394 [00:00<?, ?it/s]/share/pkg.7/pytorch/1.9.0/install/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
       0/1     7.65G   0.09133    0.2067   0.05119    0.3492        34       448: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 394/394 [01:15<00:00,  5.19it/s]

     Epoch   gpu_mem       box       obj       cls     total   targets  img_size
       1/1     7.78G   0.08605    0.1396   0.04589    0.2715        37       448: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 394/394 [01:03<00:00,  6.21it/s]
Optimizer stripped from runs/train/yolov42/weights/last.pt, 399.7MB
Optimizer stripped from runs/train/yolov42/weights/best.pt, 399.7MB
Traceback (most recent call last):
  File "./yolor/train.py", line 537, in <module>
    train(hyp, opt, device, tb_writer, wandb)
  File "./yolor/train.py", line 446, in train
    wandb.log({"Results": [wandb.Image(str(save_dir / x), caption=x) for x in
  File "./yolor/train.py", line 446, in <listcomp>
    wandb.log({"Results": [wandb.Image(str(save_dir / x), caption=x) for x in
  File "/usr4/dl523/dong760/.conda/envs/dl_env/lib/python3.8/site-packages/wandb/sdk/data_types.py", line 2109, in __init__
    self._initialize_from_path(data_or_path)
  File "/usr4/dl523/dong760/.conda/envs/dl_env/lib/python3.8/site-packages/wandb/sdk/data_types.py", line 2203, in _initialize_from_path
    self._set_file(path, is_tmp=False)
  File "/usr4/dl523/dong760/.conda/envs/dl_env/lib/python3.8/site-packages/wandb/sdk/data_types.py", line 467, in _set_file
    with open(self._path, "rb") as f:
FileNotFoundError: [Errno 2] No such file or directory: 'runs/train/yolov42/precision-recall_curve.png'

wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      metrics/mAP_0.5 ‚ñÅ‚ñÅ
wandb: metrics/mAP_0.5:0.95 ‚ñÅ‚ñÅ
wandb:    metrics/precision ‚ñÅ‚ñÅ
wandb:       metrics/recall ‚ñÅ‚ñÅ
wandb:       train/box_loss ‚ñà‚ñÅ
wandb:       train/cls_loss ‚ñà‚ñÅ
wandb:       train/obj_loss ‚ñà‚ñÅ
wandb:         val/box_loss ‚ñÅ‚ñÅ
wandb:         val/cls_loss ‚ñÅ‚ñÅ
wandb:         val/obj_loss ‚ñÅ‚ñÅ
wandb:                x/lr0 ‚ñÅ‚ñà
wandb:                x/lr1 ‚ñÅ‚ñà
wandb:                x/lr2 ‚ñà‚ñÅ
wandb: 
wandb: Run summary:
wandb:      metrics/mAP_0.5 0
wandb: metrics/mAP_0.5:0.95 0
wandb:    metrics/precision 0
wandb:       metrics/recall 0
wandb:       train/box_loss 0.08605
wandb:       train/cls_loss 0.04589
wandb:       train/obj_loss 0.13961
wandb:         val/box_loss 0
wandb:         val/cls_loss 0
wandb:         val/obj_loss 0
wandb:                x/lr0 0.00399
wandb:                x/lr1 0.00399
wandb:                x/lr2 0.03741
wandb: 
wandb: Synced yolov42: https://wandb.ai/dragogo/YOLOR/runs/1pe54vcd
wandb: Synced 5 W&B file(s), 4 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220325_233519-1pe54vcd/logs












$ python
Python 3.8.10 (default, May  3 2021, 17:15:02) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> from pycocotools import mask as maskUtils
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ModuleNotFoundError: No module named 'pycocotools'